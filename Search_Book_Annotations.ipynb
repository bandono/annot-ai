{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69976a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken  # for counting tokens\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import ast\n",
    "from scipy import spatial  # for calculating vector similarities for search strings_ranked_by_relatedness()\n",
    "from IPython.display import Markdown # for rendering answer directly in jupyter cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebe662",
   "metadata": {},
   "source": [
    "## Env\n",
    "\n",
    "Environment variables:\n",
    "1. opeanAI key\n",
    "   \n",
    "   (before starting jupyter from terminal I use something like)\n",
    "   \n",
    "   ```unset HISTFILE```\n",
    "   \n",
    "   ```export OPENAI_API_KEY=\"some-secret-key-that-you-pay-yes-you-pay\"```\n",
    "   \n",
    "\n",
    "\n",
    "2. Embedding source (now CSV file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48b26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "client = openai.OpenAI(api_key=(os.environ.get(\"OPENAI_API_KEY\")))\n",
    "\n",
    "# 2\n",
    "CSV_SAVE_PATH = \"data/everfoam_book-annotations_2024.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d831de60",
   "metadata": {},
   "source": [
    "##  1.  Load Embeddings\n",
    "\n",
    "Load from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5beee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing DataFrame\n",
    "df = pd.read_csv(CSV_SAVE_PATH, converters={'embedding': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b171614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb9f47",
   "metadata": {},
   "source": [
    "just for checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "strings, relatednesses = strings_ranked_by_relatedness(\"cynefin\", df, top_n=5)\n",
    "for string, relatedness in zip(strings, relatednesses):\n",
    "    print(f\"{relatedness=:.3f}\")\n",
    "    display(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f12cc",
   "metadata": {},
   "source": [
    "## 2. Ask\n",
    "\n",
    "Retrieve knowledge with prompting to GPT.\n",
    "\n",
    "For troubleshooting what message sent to GPT `print_message: bool = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f19c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "    introduction = 'Use the below markdown content to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nMarkdown content:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_article + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions with relevant embedding mentioning book-title\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "548fdb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- In \"Lean Software Development: An Agile Toolkit\" by Mary Poppendieck, the concept of Cynefin is discussed in relation to two schools of thought in software development: one emphasizing perfect design and code from the start, and the other advocating for small, rapid cycles of trying, testing, and fixing, especially for ill-structured problems.\n",
       "- The book \"Essential Scrum: A Practical Guide to the Most Popular Agile Process\" by Kenneth S. Rubin mentions that software development does not neatly fit into just one Cynefin domain, as it involves aspects that overlap and activities that can fall into different domains, such as complicated or complex.\n",
       "- The articles do not provide direct information on Cynefin from the other book annotations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer=ask('What information do you have have on Cynefin from annotation of books that I have created? Explain in pointers')\n",
    "\n",
    "# Render the Markdown string\n",
    "Markdown(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
